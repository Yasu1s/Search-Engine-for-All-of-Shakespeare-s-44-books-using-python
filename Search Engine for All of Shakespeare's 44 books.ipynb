{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "books_dir = 'BOOKS'\n",
        "files = os.listdir(books_dir)\n",
        "files.sort(key=lambda x: int(x.split('.')[0]))\n",
        "\n",
        "for file in tqdm(files, desc=\"Loading files\", unit=\"file\"):\n",
        "    file_path = os.path.join(books_dir, file)\n",
        "    print('\\n' + file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UktWTchVRv9W",
        "outputId": "bb025561-95d4-4ee6-eed0-d63912ae1dbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Loading files: 100%|██████████| 44/44 [00:00<00:00, 36058.89file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BOOKS/1. THE SONNETS.txt\n",
            "\n",
            "BOOKS/2. ALLS WELL THAT ENDS WELL.txt\n",
            "\n",
            "BOOKS/3. THE TRAGEDY OF ANTONY AND CLEOPATRA.txt\n",
            "\n",
            "BOOKS/4. AS YOU LIKE IT.txt\n",
            "\n",
            "BOOKS/5. THE COMEDY OF ERRORS.txt\n",
            "\n",
            "BOOKS/6. THE TRAGEDY OF CORIOLANUS.txt\n",
            "\n",
            "BOOKS/7. CYMBELINE.txt\n",
            "\n",
            "BOOKS/8. THE TRAGEDY OF HAMLET PRINCE OF DENMARK.txt\n",
            "\n",
            "BOOKS/9. THE FIRST PART OF KING HENRY THE FOURTH.txt\n",
            "\n",
            "BOOKS/10. THE SECOND PART OF KING HENRY THE FOURTH.txt\n",
            "\n",
            "BOOKS/11. THE LIFE OF KING HENRY THE FIFTH.txt\n",
            "\n",
            "BOOKS/12. THE FIRST PART OF HENRY THE SIXTH.txt\n",
            "\n",
            "BOOKS/13. THE SECOND PART OF KING HENRY THE SIXTH.txt\n",
            "\n",
            "BOOKS/14. THE THIRD PART OF KING HENRY THE SIXTH.txt\n",
            "\n",
            "BOOKS/15. KING HENRY THE EIGHTH.txt\n",
            "\n",
            "BOOKS/16. THE LIFE AND DEATH OF KING JOHN.txt\n",
            "\n",
            "BOOKS/17. THE TRAGEDY OF JULIUS CAESAR.txt\n",
            "\n",
            "BOOKS/18. THE TRAGEDY OF KING LEAR.txt\n",
            "\n",
            "BOOKS/19. LOVES LABOURS LOST.txt\n",
            "\n",
            "BOOKS/20. THE TRAGEDY OF MACBETH.txt\n",
            "\n",
            "BOOKS/21. MEASURE FOR MEASURE.txt\n",
            "\n",
            "BOOKS/22. THE MERCHANT OF VENICE.txt\n",
            "\n",
            "BOOKS/23. THE MERRY WIVES OF WINDSOR.txt\n",
            "\n",
            "BOOKS/24. A MIDSUMMER NIGHTS DREAM.txt\n",
            "\n",
            "BOOKS/25. MUCH ADO ABOUT NOTHING.txt\n",
            "\n",
            "BOOKS/26. THE TRAGEDY OF OTHELLO THE MOOR OF VENICE.txt\n",
            "\n",
            "BOOKS/27. PERICLES PRINCE OF TYRE.txt\n",
            "\n",
            "BOOKS/28. KING RICHARD THE SECOND.txt\n",
            "\n",
            "BOOKS/29. KING RICHARD THE THIRD.txt\n",
            "\n",
            "BOOKS/30. THE TRAGEDY OF ROMEO AND JULIET.txt\n",
            "\n",
            "BOOKS/31. THE TAMING OF THE SHREW.txt\n",
            "\n",
            "BOOKS/32. THE TEMPEST.txt\n",
            "\n",
            "BOOKS/33. THE LIFE OF TIMON OF ATHENS.txt\n",
            "\n",
            "BOOKS/34. THE TRAGEDY OF TITUS ANDRONICUS.txt\n",
            "\n",
            "BOOKS/35. TROILUS AND CRESSIDA.txt\n",
            "\n",
            "BOOKS/36. TWELFTH NIGHT OR WHAT YOU WILL.txt\n",
            "\n",
            "BOOKS/37. THE TWO GENTLEMEN OF VERONA.txt\n",
            "\n",
            "BOOKS/38. THE TWO NOBLE KINSMEN.txt\n",
            "\n",
            "BOOKS/39. THE WINTERS TALE.txt\n",
            "\n",
            "BOOKS/40. A LOVERS COMPLAINT.txt\n",
            "\n",
            "BOOKS/41. THE PASSIONATE PILGRIM.txt\n",
            "\n",
            "BOOKS/42. THE PHOENIX AND THE TURTLE.txt\n",
            "\n",
            "BOOKS/43. THE RAPE OF LUCRECE.txt\n",
            "\n",
            "BOOKS/44. VENUS AND ADONIS.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_files(directory):\n",
        "    files = os.listdir(directory)\n",
        "    files.sort(key=lambda x: int(x.split('.')[0]))\n",
        "    return files\n",
        "\n",
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    return words\n",
        "\n",
        "def build_word_to_files(files, directory):\n",
        "    word_to_files = defaultdict(set)\n",
        "    for file in tqdm(files, desc=\"Loading files\", unit=\"file\"):\n",
        "        file_path = os.path.join(directory, file)\n",
        "        with open(file_path, 'r') as f:\n",
        "            words = preprocess_text(f.read())\n",
        "            for word in words:\n",
        "                word_to_files[word].add(file_path)\n",
        "    return word_to_files\n",
        "\n",
        "def find_closest_word(word, word_to_files):\n",
        "    closest_word = min(word_to_files.keys(), key=lambda x: Levenshtein.distance(word, x))\n",
        "    return closest_word\n",
        "\n",
        "def process_query(query, word_to_files):\n",
        "    query_parts = query.split()\n",
        "    matching_files = set()\n",
        "\n",
        "    for i, part in enumerate(query_parts):\n",
        "        if part.lower() == 'and' and i > 0 and i < len(query_parts) - 1:\n",
        "            continue\n",
        "        elif part.lower() == 'or' and i > 0 and i < len(query_parts) - 1:\n",
        "            continue\n",
        "\n",
        "        word = preprocess_text(part)[0]\n",
        "        if word in word_to_files:\n",
        "            if i > 0 and query_parts[i - 1].lower() == 'and':\n",
        "                matching_files.intersection_update(word_to_files[word])\n",
        "            elif i > 0 and query_parts[i - 1].lower() == 'or':\n",
        "                matching_files.update(word_to_files[word])\n",
        "            else:\n",
        "                matching_files = word_to_files[word]\n",
        "        else:\n",
        "            closest_word = find_closest_word(word, word_to_files)\n",
        "            print(f\"Word '{word}' not found. Using closest match: '{closest_word}'\")\n",
        "            matching_files.update(word_to_files[closest_word])\n",
        "\n",
        "    return list(matching_files)\n",
        "\n",
        "def main():\n",
        "    books_dir = 'BOOKS'\n",
        "    files = load_files(books_dir)\n",
        "    word_to_files = build_word_to_files(files, books_dir)\n",
        "\n",
        "    query = input(\"Enter your query: \")\n",
        "    matching_files = process_query(query, word_to_files)\n",
        "\n",
        "    print(\"\\nMatching files:\")\n",
        "    for file in matching_files:\n",
        "        print(file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kywUao2VmMfP",
        "outputId": "08282c3b-a5e6-4ff5-8e3d-2c324dadf42f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading files: 100%|██████████| 44/44 [00:24<00:00,  1.82file/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: hamlet AND prince\n",
            "\n",
            "Matching files:\n",
            "BOOKS/8. THE TRAGEDY OF HAMLET PRINCE OF DENMARK.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YKo6cWs_m7d0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}